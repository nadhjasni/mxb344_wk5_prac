---
title: "MXB344 Week 5 Practical Logistic Regression"
author: "Miles McBain"
date: "24 August 2016"
output: html_document
---

#Intro
In this session we're going to introduce some logistic regression for predictive modelling using the titanic data from the Kaggle website. We'll see how accurately we can predict passenger survival based of data from the ship's manifest. We can submit our results to Kaggle for scoring against the public leader board.

#Learning Objectives
The exercise aims to give you the chance to apply binary logistic regression in the context of trying to produce an optimal predictive model. Concepts related to predictive performance such as the ROC curve, Confusion Matrix, Cross validation etc will be introduced applied.

#Requirements
To complete this exercise you will need a computer with R Studio installed and the following packages:

* `dplyr`
* `tidyr`
* `ggplot2`
* `readr`
* `rmarkdown`
* `caret`
* `ROCR`


#Instructions
Unlike the last practicals this repository does not contain the data. To get it you will need to head over [here](https://www.kaggle.com/c/titanic) and create a kaggle account.

## Get Set Up
Assuming you are on a QUT windows PC:

1. Install missing R packages in RStudio.
2. Fork the practical repository on github.com from [here](https://github.com/MilesMcBain/MXB344_wk5_prac)
3. Install the github windows client from [https://desktop.github.com/](https://desktop.github.com/)
4. Clone your fork of the prac to a directory on your workstation's hard disk.
5. Open a local version of this source file.
7. knit this file to html.



## Load the data
Predictive modelling uses the concept of hold-out data sets for avoiding over fitting. Briefly, this means we build our model using the train set and test its performance on the test set. Kaggle have 1 further hold out set that we will be trying to make accurate predictions for. We will discuss this further in the practical session.

```{r, echo=FALSE, include=FALSE}

library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(rmarkdown)
library(caret)
library(ROCR)
#train <- read_csv("../input/train.csv")
#test  <- read_csv("../input/test.csv")
```
#Practical

## Exploratory Analysis
Explore relationships using plots and summary stats.

ggplot(data=train) + geom_bar(aes(x=Pclass, fill=factor(Survived)),position="dodge")

There are more people died in class 3, whilst class 1 has the highest number of people survive.

ggplot(data=train) + geom_bar(aes(x=Sex, fill=factor(Survived)),position="dodge")
Male passenger has higher number of people died compared to female. 

Inference : Logically, women and children are given more priority  to survive under life-threatening situation

table(as.factor(train$Survived),as.factor(train$Embarked))

the place they embark 

## Logistic Regression Model selection

### Data Splitting
`Use caret::createDataPartition()` to create some data splits for 'training', 'validation', and 'testing'. Use a 60/20/20% ratio initially. This is a type of  **Cross Validation**. Cross validation procedures mainly guard against over fitting by evaluating models on hold-out data. 

```{r}
train_split <- caret::createDataPartition(
               y=train$Survived,p=0.6,list=F)
               
train_set <- train[train_split,]  
val_set <- train[-train_split,]
```

### Model Fitting
Fit a first pass binary glm model based on your exploratory analysis. A good one to start with would be `survival ~ pclass + gender`.

```{r}
surv <- glm( formula = Survived ~ Pclass + Sex, family= binomial(link = "logit"),data=train_set)
```

## Model Validation
plot(surv)
* Check the residuals and see why they're not helpful for binary models.

## Model performance 
For training and validation sets:
* Obtain the confusion matrix for your model using `caret::confusionMatrix()`
  + How do we interpret the measures in the confusion matrix output?
  + What pitfalls can they help us avoid?

```{r}
train_preds <- predicts(surv, type = "response")
train_preds[train_preds > 0.5] <- 1
train_preds[train_preds <=0.5] <- 0

caret::confusionMatrix(data = train_preds, reference = train_set$Survived)

val_preds <- predict(binom_fit1,type = "response", newdata = val_set)
val_preds[val_preds > 0.5] <- 1
val_preds[val_preds <= 0.5] <- 0

caret::confusionMatrix(data = val_preds, reference = val_set$Survived)
```

* Plot a ROC curve using `ROCR::predictions()` followed by `ROCR::performance()`
How does your model perform on the test set compared to the training set? How could you improve it? Try an alternate link function.

```{r}
library(ROCR)
val_preds_p <- predict(binom_fit1,type = "response", newdata = val_set)
val_pred_ob <- prediction(predictions = val_preds_p, labels = val_set$Survived)
perf <- performance(val_pred_ob, measure = "tpr", x.measure = "fpr")
plot(perf)

val_gini = 2*(performance(val_pred_ob, measure = "auc")@y.values[[1]] - 0.5)
```

##Improving the model
* To improve the model we manipulate the data to get more information out of it. Some things to try:
    + Create a covariate for a person's title e.g.: Mrs, Ms, Mr, Dr, etc.
    + Create a covariate for whether the person is a mother.
    + Devise a scheme to impute (guess) missing age values. What other data might be correlated with Age?
* This process is called **Feature Engineering**

##Evaluate Improved model on test set.
* The test set performance tells us how we expect our model to perform on the hold out data used by the Kaggle Scoring system. Evaluate your model's performance and note it for comparison with Kaggle's score.

## Make a submission to Kaggle to get scored!
https://www.kaggle.com/c/titanic/submissions/attach
* Did you model perform as per the expectations set by the test set?
* Do we really need the test set in this case, if there is hold-out data being used for the leader board?
##Extensions
There are some R tutorials for this data set ![here](https://www.kaggle.com/c/titanic/details/new-getting-started-with-r) that cover some of the topics we have discussed in this practical and more.

## Commit your work and push it to Github
:)